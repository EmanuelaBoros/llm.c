{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b223b0-ad44-413d-9bcb-35e79dc9635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parquet_file = \"../data/gallica_presse_1_1.parquet\"\n",
    "\n",
    "data = pd.read_parquet(parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602ad5b8-cece-481e-8b85-83e79c7b20b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/p11/lib/python3.11/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/envs/p11/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/p11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/p11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/p11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/p11/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Downloading tiktoken-0.8.0-cp311-cp311-macosx_11_0_arm64.whl (982 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m982.4/982.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3299ae1a-e24c-4d21-8e72-8ad3eea354e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_id</th>\n",
       "      <th>file_id</th>\n",
       "      <th>ocr</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>page_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>character_count</th>\n",
       "      <th>text</th>\n",
       "      <th>corrected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>940</td>\n",
       "      <td>bpt6k6441992x</td>\n",
       "      <td>99</td>\n",
       "      <td>Journal officiel de la République française. D...</td>\n",
       "      <td>1902-01-20</td>\n",
       "      <td>None</td>\n",
       "      <td>112</td>\n",
       "      <td>58622</td>\n",
       "      <td>356947</td>\n",
       "      <td>\\nCHAMBRE DES DÉPUTÉS 71 législature. Session ...</td>\n",
       "      <td>CHAMBRE DES DÉPUTÉS 71e législature. Session o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>941</td>\n",
       "      <td>bpt6k2353312n</td>\n",
       "      <td>86</td>\n",
       "      <td>Journal de la Manche et de la Basse-Normandie ...</td>\n",
       "      <td>1913-03-01</td>\n",
       "      <td>None</td>\n",
       "      <td>22</td>\n",
       "      <td>31657</td>\n",
       "      <td>188831</td>\n",
       "      <td>\\nDixième Année.— N©960 \\nDIX CENTIMES LE NUME...</td>\n",
       "      <td>Dixième Année.— 1960 \\nDIX CENTIMES LE NUMÉRO ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>942</td>\n",
       "      <td>bpt6k605149p</td>\n",
       "      <td>99</td>\n",
       "      <td>Le Petit Parisien : journal quotidien du soir</td>\n",
       "      <td>1922-09-22</td>\n",
       "      <td>None</td>\n",
       "      <td>22</td>\n",
       "      <td>27888</td>\n",
       "      <td>175974</td>\n",
       "      <td>\\nLES DÉLIBÉRATIONS \\nSORUlUESDID'lEir reprenn...</td>\n",
       "      <td>LES DÉLIBÉRATIONS \\nSOURCUES DID'ELIRE reprenn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>943</td>\n",
       "      <td>bpt6k54904729</td>\n",
       "      <td>93</td>\n",
       "      <td>Bulletin du Photo-club de Constantine...</td>\n",
       "      <td>1894-12</td>\n",
       "      <td>Photo club de Constantine</td>\n",
       "      <td>34</td>\n",
       "      <td>2715</td>\n",
       "      <td>17059</td>\n",
       "      <td>\\nDeuxième année \\nDÉCEMBRE 1894 \\nN° 16 \\nFou...</td>\n",
       "      <td>Deuxième année\\nDÉCEMBRE 1894\\nN° 16\\nPour les...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>944</td>\n",
       "      <td>bpt6k611668p</td>\n",
       "      <td>99</td>\n",
       "      <td>L'Ouest-Éclair</td>\n",
       "      <td>1926-07-29</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>51163</td>\n",
       "      <td>317488</td>\n",
       "      <td>\\nL'UNION SACREE POUR LE TRAVAIL \\nNotre colla...</td>\n",
       "      <td>L'UNION SACRÉE POUR LE TRAVAIL \\nNotre collabo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_id        file_id ocr  \\\n",
       "0       940  bpt6k6441992x  99   \n",
       "1       941  bpt6k2353312n  86   \n",
       "2       942   bpt6k605149p  99   \n",
       "3       943  bpt6k54904729  93   \n",
       "4       944   bpt6k611668p  99   \n",
       "\n",
       "                                               title        date  \\\n",
       "0  Journal officiel de la République française. D...  1902-01-20   \n",
       "1  Journal de la Manche et de la Basse-Normandie ...  1913-03-01   \n",
       "2      Le Petit Parisien : journal quotidien du soir  1922-09-22   \n",
       "3           Bulletin du Photo-club de Constantine...     1894-12   \n",
       "4                                     L'Ouest-Éclair  1926-07-29   \n",
       "\n",
       "                      author  page_count  word_count  character_count  \\\n",
       "0                       None         112       58622           356947   \n",
       "1                       None          22       31657           188831   \n",
       "2                       None          22       27888           175974   \n",
       "3  Photo club de Constantine          34        2715            17059   \n",
       "4                       None          40       51163           317488   \n",
       "\n",
       "                                                text  \\\n",
       "0  \\nCHAMBRE DES DÉPUTÉS 71 législature. Session ...   \n",
       "1  \\nDixième Année.— N©960 \\nDIX CENTIMES LE NUME...   \n",
       "2  \\nLES DÉLIBÉRATIONS \\nSORUlUESDID'lEir reprenn...   \n",
       "3  \\nDeuxième année \\nDÉCEMBRE 1894 \\nN° 16 \\nFou...   \n",
       "4  \\nL'UNION SACREE POUR LE TRAVAIL \\nNotre colla...   \n",
       "\n",
       "                                      corrected_text  \n",
       "0  CHAMBRE DES DÉPUTÉS 71e législature. Session o...  \n",
       "1  Dixième Année.— 1960 \\nDIX CENTIMES LE NUMÉRO ...  \n",
       "2  LES DÉLIBÉRATIONS \\nSOURCUES DID'ELIRE reprenn...  \n",
       "3  Deuxième année\\nDÉCEMBRE 1894\\nN° 16\\nPour les...  \n",
       "4  L'UNION SACRÉE POUR LE TRAVAIL \\nNotre collabo...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a947c6-5967-4dae-b601-7f48fad434a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "HEADERS_INFO = {\n",
    "    \"gpt-2\": {\n",
    "        \"magic\": 20240520,\n",
    "        \"version\": 1,\n",
    "        \"token_dtype\": np.uint16,\n",
    "    },\n",
    "    \"llama-3\": {\n",
    "        \"magic\": 20240801,\n",
    "        \"version\": 7,\n",
    "        \"token_dtype\": np.uint32,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6f86e4-ad0f-496a-abee-e62dcbc7b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_datafile(filename, toks, model_desc=\"gpt-2\"):\n",
    "    \"\"\"\n",
    "    Saves token data as a .bin file, for reading in C.\n",
    "    - First comes a header with 256 int32s\n",
    "    - The tokens follow, each as uint16 (gpt-2) or uint32 (llama)\n",
    "    \"\"\"\n",
    "    assert len(toks) < 2**31, \"token count too large\" # ~2.1B tokens\n",
    "    assert model_desc in [\"gpt-2\", \"llama-3\"], f\"unknown model descriptor {model_desc}\"\n",
    "    info = HEADERS_INFO[model_desc]\n",
    "    # construct the header\n",
    "    header = np.zeros(256, dtype=np.int32) # header is always 256 int32 values\n",
    "    header[0] = info[\"magic\"]\n",
    "    header[1] = info[\"version\"]\n",
    "    header[2] = len(toks) # number of tokens after the 256*4 bytes of header\n",
    "    # construct the data (numpy array of tokens)\n",
    "    toks_np = np.array(toks, dtype=info[\"token_dtype\"])\n",
    "    # write to file\n",
    "    num_bytes = (256 * 4) + (len(toks) * toks_np.itemsize)\n",
    "    print(f\"writing {len(toks):,} tokens to {filename} ({num_bytes:,} bytes) in the {model_desc} format\")\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(header.tobytes())\n",
    "        f.write(toks_np.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af5df84-6cd4-4498-81d9-711cd4a679ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Cleaning text data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OCR Text: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2447/2447 [00:23<00:00, 103.57it/s]\n",
      "Processing Corrected Text: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2447/2447 [00:20<00:00, 121.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting Text: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2447/2447 [00:00<00:00, 5190.53it/s]\n",
      "Saving formatted text: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2447/2447 [00:00<00:00, 3145.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted data saved to ../data/processed_ocr_data/ocr_text_data.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "import struct\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define constants\n",
    "DATA_DIR = \"../data/processed_ocr_data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# 1. Load and Clean OCR Data\n",
    "def load_and_clean_data(parquet_file):\n",
    "    \"\"\"Load and clean OCR data with progress tracking.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    # Drop rows with missing text\n",
    "    df.dropna(subset=['text', 'corrected_text'], inplace=True)\n",
    "\n",
    "    # Clean text function with progress bar\n",
    "    def clean_text(text):\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines\n",
    "        text = re.sub(r'[^\\x20-\\x7E]', '', text)  # Remove non-printable ASCII characters\n",
    "        return text\n",
    "\n",
    "    print(\"Cleaning text data...\")\n",
    "    df['text'] = [clean_text(text) for text in tqdm(df['text'], desc=\"Processing OCR Text\")]\n",
    "    df['corrected_text'] = [clean_text(text) for text in tqdm(df['corrected_text'], desc=\"Processing Corrected Text\")]\n",
    "\n",
    "    # Combine text pairs\n",
    "    print(\"Formatting data...\")\n",
    "    df['formatted'] = [f\"### Text ###\\n{text}\\n\\n\\n### Correction ###\\n{corr_text}\\n\" for text, corr_text in tqdm(zip(df['text'], df['corrected_text']), total=len(df), desc=\"Formatting Text\")]\n",
    "\n",
    "    # Save formatted text to a file\n",
    "    text_filename = os.path.join(DATA_DIR, \"ocr_text_data.txt\")\n",
    "    with open(text_filename, 'w', encoding='utf-8') as f:\n",
    "        for line in tqdm(df['formatted'], desc=\"Saving formatted text\"):\n",
    "            f.write(line + \"\\n\")\n",
    "    \n",
    "    print(f\"Formatted data saved to {text_filename}\")\n",
    "    return text_filename\n",
    "\n",
    "# 2. Tokenization Function\n",
    "def tokenize_ocr_data(text_filename, model_desc=\"gpt-2\"):\n",
    "    \"\"\"Tokenize text data and split into training/validation sets with progress tracking.\"\"\"\n",
    "    print(f\"Tokenizing data using {model_desc} model...\")\n",
    "\n",
    "    if model_desc == \"gpt-2\":\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        encode = lambda s: enc.encode_ordinary(s)\n",
    "        eot = enc._special_tokens['<|endoftext|>']  # End of text token for GPT-2\n",
    "        token_size = 2  # GPT-2 tokens use uint16 (2 bytes)\n",
    "    elif model_desc == \"llama-3\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "        encode = lambda s: tokenizer.encode(s, add_special_tokens=False)\n",
    "        eot = tokenizer.encode('')[0]  # LLaMA end-of-text token\n",
    "        token_size = 4  # LLaMA tokens use uint32 (4 bytes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model descriptor {model_desc}\")\n",
    "\n",
    "    # Read the text\n",
    "    with open(text_filename, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split into sections and tokenize\n",
    "    sections = text.split(\"\\n\\n\")\n",
    "    tokens = []\n",
    "    \n",
    "    print(\"Tokenizing text sections...\")\n",
    "    for i, s in tqdm(enumerate(sections), total=len(sections), desc=\"Tokenizing\"):\n",
    "        tokens.append(eot)\n",
    "        spad = s + \"\\n\\n\" if i != len(sections) - 1 else s\n",
    "        tokens.extend(encode(spad))\n",
    "\n",
    "    # Split into training (90%) and validation (10%)\n",
    "    val_tokens = tokens[:32768]\n",
    "    train_tokens = tokens[32768:]\n",
    "\n",
    "    # Print token counts\n",
    "    print(f\"Total Tokens: {len(tokens)}\")\n",
    "    print(f\"Training Tokens: {len(train_tokens)}\")\n",
    "    print(f\"Validation Tokens: {len(val_tokens)}\")\n",
    "\n",
    "    # Save tokenized data\n",
    "    val_filename = os.path.join(DATA_DIR, f\"ocr_val_{model_desc}.bin\")\n",
    "    train_filename = os.path.join(DATA_DIR, f\"ocr_train_{model_desc}.bin\")\n",
    "\n",
    "    save_tokens(val_filename, val_tokens, token_size)\n",
    "    save_tokens(train_filename, train_tokens, token_size)\n",
    "\n",
    "    print(f\"Tokenized data saved to {val_filename} and {train_filename}\")\n",
    "\n",
    "# 3. Save Tokenized Data\n",
    "def save_tokens(filename, token_list, token_size):\n",
    "    \"\"\"Save tokenized data in binary format with progress tracking.\"\"\"\n",
    "    print(f\"Saving tokenized data to {filename}...\")\n",
    "    with open(filename, 'wb') as f:\n",
    "        for token in tqdm(token_list, desc=f\"Writing to {filename}\"):\n",
    "            f.write(struct.pack(f\"<{token_size}s\", token.to_bytes(token_size, byteorder=\"little\")))\n",
    "\n",
    "text_file = load_and_clean_data(parquet_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee2455-568f-4512-907f-06548c457147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize and split the data (choose 'gpt-2' or 'llama-3')\n",
    "tokenize_ocr_data(text_file, model_desc=\"gpt-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdfe711-2da0-4043-8e71-2b54bee492a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36212ff-bbf5-4dcc-ba56-3b0e1654de54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e98f7c-b21c-4f3b-98e6-ccae5f1def93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c67dc9-2f5b-492b-af75-07305473d21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7502c-e403-4c42-99c2-d0774afc5116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (p11)",
   "language": "python",
   "name": "p11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
